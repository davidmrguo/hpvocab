{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "# Import and download packages\n",
                "import pandas as pd\n",
                "import glob\n",
                "import nltk\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "import codecs\n",
                "import json\n",
                "nltk.download('punkt')\n",
                "nltk.download('wordnet')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package punkt to /Users/davidguo/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     /Users/davidguo/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 1
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# Read CSV into DataFrame\n",
                "df = pd.read_csv('./middle_school_vocab.csv')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Clean up DataFrame\n",
                "df['in_text'] = 0\n",
                "df['word'] = df['word'].str.lower()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# Read text files\n",
                "read_files = glob.glob(\"hp_text/*[1-7].txt\")\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "word_list = []\n",
                "for file_path in read_files:\n",
                "    with codecs.open(file_path, 'r', encoding='utf-8',\n",
                "                 errors='ignore') as file_object:\n",
                "        content = file_object.read()\n",
                "        words_in_content = nltk.word_tokenize(content)\n",
                "        for word in words_in_content:\n",
                "            lemmatizer = WordNetLemmatizer()\n",
                "            word_list.append(lemmatizer.lemmatize(word).lower())\n",
                "deduped_word_list = list(set(word_list))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "i_max = df.shape[0]\n",
                "i = 0\n",
                "j = 0\n",
                "\n",
                "with open('b2a.json') as f:\n",
                "    b2a_dict = json.load(f)\n",
                "\n",
                "    while i < i_max:\n",
                "        loc_word = df['word'].loc[i]\n",
                "        loc_word_2 = lemmatizer.lemmatize(loc_word).lower()\n",
                "        loc_word_3 = b2a_dict.get(loc_word_2)\n",
                "        if loc_word_2 in deduped_word_list:\n",
                "            df.at[i, 'in_text'] = 1\n",
                "            j += 1\n",
                "        elif loc_word_3 in deduped_word_list:\n",
                "            df.at[i, 'in_text'] = 1\n",
                "            j += 1\n",
                "        i += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "output = str(round(j / i_max * 100, 2)) + \"%\"\n",
                "print(output)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "94.62%\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "df_not_in_text = df.query('in_text == 0')\n",
                "df_not_in_text.to_csv('not_in_text.csv', index=False)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "c63ef0e16bd3068cbaba0bd7cebd4f043c481c6312de4cb279d9ee86ddd6d348"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}