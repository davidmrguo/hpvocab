{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "# Import and download packages\n",
                "import pandas as pd\n",
                "import glob\n",
                "import nltk\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "import codecs\n",
                "import json\n",
                "nltk.download('punkt')\n",
                "nltk.download('wordnet')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package punkt to /Users/davidguo/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     /Users/davidguo/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 1
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# Read CSV into DataFrame\n",
                "df = pd.read_csv('./word_list/middle_school_vocab.csv')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Clean up DataFrame\n",
                "df['in_text'] = 0\n",
                "df['word'] = df['word'].str.lower()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# Read text files\n",
                "hp_files = glob.glob(\"hp_text/*[1-7].txt\")\n",
                "\n",
                "# Load BrE to AmE dictionary\n",
                "b2a_dict = json.load(open('b2a.json'))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "word_list = []\n",
                "for file in hp_files:\n",
                "    with codecs.open(file, 'r', encoding='utf-8',\n",
                "                 errors='ignore') as file_object:\n",
                "        content = file_object.read()\n",
                "        words_in_content = nltk.word_tokenize(content)\n",
                "        for word in words_in_content:\n",
                "            ame_word = b2a_dict.get(word)\n",
                "            lemmatizer = WordNetLemmatizer()\n",
                "            if ame_word:                \n",
                "                word_list.append(lemmatizer.lemmatize(ame_word).lower())\n",
                "            else:\n",
                "                word_list.append(lemmatizer.lemmatize(word).lower())\n",
                "deduped_word_list = list(set(word_list))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "i_max = df.shape[0]\n",
                "i = 0\n",
                "j = 0\n",
                "\n",
                "while i < i_max:\n",
                "    lemmatizer = WordNetLemmatizer()\n",
                "    loc_word = df['word'].loc[i]\n",
                "    lem_word = lemmatizer.lemmatize(loc_word).lower()\n",
                "    ame_word = b2a_dict.get(lem_word)\n",
                "    if lem_word in deduped_word_list:\n",
                "        df.at[i, 'in_text'] = 1\n",
                "        j += 1\n",
                "    elif ame_word in deduped_word_list:\n",
                "        df.at[i, 'in_text'] = 1\n",
                "        j += 1\n",
                "    i += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "output = str(round(j / i_max * 100, 2)) + \"%\"\n",
                "print(output)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "79.46%\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "df_not_in_text = df.query('in_text == 0')\n",
                "df_not_in_text.to_csv('not_in_text.csv', index=False)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "c63ef0e16bd3068cbaba0bd7cebd4f043c481c6312de4cb279d9ee86ddd6d348"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}